{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "835b1cfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:59: error: ambiguous reference to overloaded definition,\nboth method window in object functions of type (timeColumn: org.apache.spark.sql.Column, windowDuration: String)org.apache.spark.sql.Column\nand  method window in object functions of type (timeColumn: org.apache.spark.sql.Column, windowDuration: String, slideDuration: String)org.apache.spark.sql.Column\nmatch expected type ?\n       val outputDf=windowDf.select(window.start,window.end,Total Invoice)\n                                    ^\n<console>:59: error: ambiguous reference to overloaded definition,\nboth method window in object functions of type (timeColumn: org.apache.spark.sql.Column, windowDuration: String)org.apache.spark.sql.Column\nand  method window in object functions of type (timeColumn: org.apache.spark.sql.Column, windowDuration: String, slideDuration: String)org.apache.spark.sql.Column\nmatch expected type ?\n       val outputDf=windowDf.select(window.start,window.end,Total Invoice)\n                                                 ^\n<console>:59: error: not found: value Total\n       val outputDf=windowDf.select(window.start,window.end,Total Invoice)\n                                                            ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// Requiring all the Imports\n",
    "import org.apache.spark.streaming.StreamingContext\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.streaming.Seconds\n",
    "import org.apache.log4j.Level\n",
    "import org.apache.spark.sql.SparkSession\n",
    "// creating a Spark Session\n",
    "val spark=SparkSession.builder().master(\"local[2]\").appName(\"MyStreamingApplication\")\n",
    ".config(\"spark.streaming.stopGracefullyOnShutdown\",true)\n",
    ".config(\"spark.sql.shuffle.partitions\",4)\n",
    ".getOrCreate()\n",
    "\n",
    "//read the data from socket\n",
    "val linesDf=spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",\"6767\").load()\n",
    "\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "val orderSchema=StructType(List(\n",
    "StructField(\"order_id\",IntegerType),\n",
    "StructField(\"order_date\",TimestampType),\n",
    "StructField(\"order_customer_id\",IntegerType),\n",
    "StructField(\"order_status\",StringType),\n",
    "StructField(\"order_amount\",IntegerType)))\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// procesing of data\n",
    "val valueDf=linesDf.select(from_json(col(\"value\"),orderSchema).alias(\"value\"))\n",
    "valueDf.printSchema()\n",
    "//val redefinedDf=valueDf.select(\"value\")\n",
    "\n",
    "val finalDf=valueDf.select(\"value.*\")\n",
    "val windowDf=finalDf.groupBy(window(col(\"order_date\"),\"15 minute\")).agg(sum(\"order_amount\").alias(\"Total Invoice\"))\n",
    "val outputDf=windowDf.select(window.start,window.end,Total Invoice)\n",
    "val orders=outputDf.writeStream.format(\"console\").outputMode(\"update\")option(\"checkpointLocation\",\"Location1\").trigger(Trigger.processingTime(\"15 second\")).start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf39712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2 - Scala",
   "language": "scala",
   "name": "spark_2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

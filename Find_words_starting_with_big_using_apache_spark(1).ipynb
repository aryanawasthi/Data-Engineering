{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b90210",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Program to find the words which start with the letter \"Big\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dccb980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1669274380000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669274385000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669274390000 ms\n",
      "-------------------------------------------\n",
      "bigdata\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669274395000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669274400000 ms\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// importing all the required libraries\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.StreamingContext._\n",
    "\n",
    "// starting the streaming session using local\n",
    "val sc=new SparkContext(\"local[2]\",\"wordcount\")\n",
    "// starting the stream session \n",
    "val ssc=new StreamingContext(sc,Seconds(5))\n",
    "// connecting to the producer\n",
    "val lines=ssc.socketTextStream(\"localhost\",9990)\n",
    "// collecting the rdd into the current directory\n",
    "ssc.checkpoint(\".\")\n",
    "// splitting all the data on the basis of spaces \n",
    "val data=lines.flatMap(x=>x.split(\" \"))\n",
    "// filtering the data on the basis of whether the words start with \"big\"\n",
    "val words_start_with_map=data.filter(x=>x.startsWith(\"big\"))\n",
    "// collecting the results and taking action\n",
    "words_start_with_map.print()\n",
    "// starts the streaming service\n",
    "ssc.start()\n",
    "// starts the awaiting termination\n",
    "ssc.awaitTermination()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b854549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1669276145000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669276150000 ms\n",
      "-------------------------------------------\n",
      "big\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669276155000 ms\n",
      "-------------------------------------------\n",
      "big\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669276160000 ms\n",
      "-------------------------------------------\n",
      "big\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1669276165000 ms\n",
      "-------------------------------------------\n",
      "big\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// solving the same issue using stateful transformation\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.StreamingContext._\n",
    "\n",
    "// starting the streaming session using local\n",
    "val sc=new SparkContext(\"local[2]\",\"wordcount\")\n",
    "// starting the stream session \n",
    "val ssc=new StreamingContext(sc,Seconds(5))\n",
    "// connecting to the producer\n",
    "val lines=ssc.socketTextStream(\"localhost\",9990)\n",
    "// collecting the rdd into the current directory\n",
    "ssc.checkpoint(\".\")\n",
    "// splitting all the data on the basis of spaces \n",
    "val data=lines.flatMap(x=>x.split(\" \"))\n",
    "//aryan aman vimlesh kamal \n",
    "// saurah big data \n",
    "val words=data.map(x=>(x,1))\n",
    "// input to the function is (aryan,1) (aman,1) (vimlesh,1)   --  (aryan,1),(vimlesh,1)  --output is  (aryan,2) (vimlesh,2)\n",
    "def update_func(newValues:Seq[Int],previousState:Option[Int]):Option[Int]={\n",
    "    val new_count=previousState.getOrElse(0)+newValues.sum\n",
    "    Some(new_count)\n",
    "    \n",
    "    \n",
    "}\n",
    "val new_data=words.updateStateByKey(update_func)\n",
    "val single_data=new_data.map(x=>(x._1))\n",
    "val words_start_with_map=single_data.filter(x=>x.startsWith(\"big\"))\n",
    "// collecting the results and taking action\n",
    "words_start_with_map.print()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269dd608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f93bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2 - Scala",
   "language": "scala",
   "name": "spark_2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
